{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f408828",
   "metadata": {},
   "source": [
    "## GPT-2 GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d42347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded from D:\\me\\College\\3rd year 2nd term\\Pattern\\final project\\gpt2_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import GPT2Tokenizer\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# 2. Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        return output\n",
    "\n",
    "# 3. Feed-Forward Neural Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "# 4. Transformer Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# 5. GPT-2 Model\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=768, num_layers=2, num_heads=12, d_ff=3072, max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def generate(self, input_ids, max_length=50, method=\"greedy\"):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                mask = torch.tril(torch.ones(input_ids.size(1), input_ids.size(1), device=device)).unsqueeze(0).unsqueeze(0)\n",
    "                logits = self(input_ids, mask)[:, -1, :]\n",
    "                if method == \"greedy\":\n",
    "                    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "        return input_ids\n",
    "\n",
    "# 6. Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 7. Load Model\n",
    "def load_model(model, load_path):\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from {load_path}\")\n",
    "    return model\n",
    "\n",
    "# 8. Generate Text\n",
    "def generate_text(model, prompt, max_length=50, method=\"greedy\"):\n",
    "    try:\n",
    "        model.eval()\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        generated_ids = model.generate(input_ids, max_length, method)\n",
    "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"Error generating text: {str(e)}\"\n",
    "\n",
    "# GUI Application\n",
    "class GPT2GUI:\n",
    "    def __init__(self, root, model):\n",
    "        self.root = root\n",
    "        self.model = model\n",
    "        self.root.title(\"GPT-2 Text Generator\")\n",
    "        self.root.geometry(\"600x500\")\n",
    "        self.root.configure(bg=\"#E6F3FF\")  # Light blue background\n",
    "\n",
    "        # Center the window\n",
    "        self.root.update_idletasks()\n",
    "        width = self.root.winfo_width()\n",
    "        height = self.root.winfo_height()\n",
    "        x = (self.root.winfo_screenwidth() // 2) - (width // 2)\n",
    "        y = (self.root.winfo_screenheight() // 2) - (height // 2)\n",
    "        self.root.geometry(f\"{width}x{height}+{x}+{y}\")\n",
    "\n",
    "        # Title Label\n",
    "        title_label = tk.Label(\n",
    "            root, text=\"GPT-2 Text Generator\", font=(\"Helvetica\", 16, \"bold\"),\n",
    "            bg=\"#E6F3FF\", fg=\"#003087\"  # Dark blue text\n",
    "        )\n",
    "        title_label.pack(pady=10)\n",
    "\n",
    "        # Prompt Label and Entry\n",
    "        prompt_label = tk.Label(\n",
    "            root, text=\"Enter Prompt:\", font=(\"Helvetica\", 12),\n",
    "            bg=\"#E6F3FF\", fg=\"#003087\"\n",
    "        )\n",
    "        prompt_label.pack()\n",
    "        self.prompt_entry = tk.Entry(\n",
    "            root, width=50, font=(\"Helvetica\", 10),\n",
    "            bg=\"white\", fg=\"black\", bd=2, relief=\"groove\"\n",
    "        )\n",
    "        self.prompt_entry.insert(0, \"I have a cat\")  # Default prompt\n",
    "        self.prompt_entry.pack(pady=5)\n",
    "\n",
    "        # Output Text Area with Scrollbar\n",
    "        self.output_text = scrolledtext.ScrolledText(\n",
    "            root, width=60, height=15, font=(\"Helvetica\", 10),\n",
    "            bg=\"white\", fg=\"black\", bd=2, relief=\"groove\", wrap=tk.WORD\n",
    "        )\n",
    "        self.output_text.pack(pady=10)\n",
    "        self.output_text.config(state=\"disabled\")  # Read-only initially\n",
    "\n",
    "        # Button Frame\n",
    "        button_frame = tk.Frame(root, bg=\"#E6F3FF\")\n",
    "        button_frame.pack(pady=10)\n",
    "\n",
    "        # Button Styles\n",
    "        button_style = {\n",
    "            \"font\": (\"Helvetica\", 10, \"bold\"), \"relief\": \"raised\", \"bd\": 3,\n",
    "            \"width\": 12, \"cursor\": \"hand2\"\n",
    "        }\n",
    "\n",
    "        # New Prompt Button\n",
    "        self.new_button = tk.Button(\n",
    "            button_frame, text=\"New Prompt\", command=self.new_prompt,\n",
    "            bg=\"#4CAF50\", fg=\"white\", **button_style\n",
    "        )\n",
    "        self.new_button.grid(row=0, column=0, padx=5)\n",
    "        self.new_button.bind(\"<Enter>\", lambda e: self.new_button.config(bg=\"#45a049\"))\n",
    "        self.new_button.bind(\"<Leave>\", lambda e: self.new_button.config(bg=\"#4CAF50\"))\n",
    "\n",
    "        # Generate Button\n",
    "        self.generate_button = tk.Button(\n",
    "            button_frame, text=\"Generate\", command=self.generate,\n",
    "            bg=\"#2196F3\", fg=\"white\", **button_style\n",
    "        )\n",
    "        self.generate_button.grid(row=0, column=1, padx=5)\n",
    "        self.generate_button.bind(\"<Enter>\", lambda e: self.generate_button.config(bg=\"#1e88e5\"))\n",
    "        self.generate_button.bind(\"<Leave>\", lambda e: self.generate_button.config(bg=\"#2196F3\"))\n",
    "\n",
    "        # Close Button\n",
    "        self.close_button = tk.Button(\n",
    "            button_frame, text=\"Close\", command=self.close,\n",
    "            bg=\"#F44336\", fg=\"white\", **button_style\n",
    "        )\n",
    "        self.close_button.grid(row=0, column=2, padx=5)\n",
    "        self.close_button.bind(\"<Enter>\", lambda e: self.close_button.config(bg=\"#e53935\"))\n",
    "        self.close_button.bind(\"<Leave>\", lambda e: self.close_button.config(bg=\"#F44336\"))\n",
    "\n",
    "        # Footer Label\n",
    "        footer_label = tk.Label(\n",
    "            root, text=\"GPT-2 Tiny Stories\", font=(\"Helvetica\", 10, \"italic\"),\n",
    "            bg=\"#E6F3FF\", fg=\"#003087\"\n",
    "        )\n",
    "        footer_label.pack(side=\"bottom\", pady=5)\n",
    "\n",
    "    def new_prompt(self):\n",
    "        self.prompt_entry.delete(0, tk.END)\n",
    "        self.prompt_entry.insert(0, \"\")\n",
    "        self.output_text.config(state=\"normal\")\n",
    "        self.output_text.delete(\"1.0\", tk.END)\n",
    "        self.output_text.config(state=\"disabled\")\n",
    "\n",
    "    def generate(self):\n",
    "        prompt = self.prompt_entry.get().strip()\n",
    "        if not prompt:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter a prompt!\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            generated_text = generate_text(self.model, prompt, max_length=50, method=\"greedy\")\n",
    "            self.output_text.config(state=\"normal\")\n",
    "            self.output_text.delete(\"1.0\", tk.END)\n",
    "            self.output_text.insert(tk.END, f\"Prompt: {prompt}\\n\\nGenerated Text: {generated_text}\")\n",
    "            self.output_text.config(state=\"disabled\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to generate text: {str(e)}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.root.destroy()\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    model = GPT2(vocab_size=vocab_size, d_model=768, num_layers=2, num_heads=12, d_ff=3072, max_len=256).to(device)\n",
    "\n",
    "    # Load saved model\n",
    "    load_path = r\"D:\\me\\College\\3rd year 2nd term\\Pattern\\final project\\gpt2_model.pth\" # Put the model path here.\n",
    "    try:\n",
    "        model = load_model(model, load_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Create GUI\n",
    "    root = tk.Tk()\n",
    "    app = GPT2GUI(root, model)\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
